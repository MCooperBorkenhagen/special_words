```{r WordsInLowerLeft}

rank_inconsistency_rank_frequency = #tasa %>% 
  #filter(word %in% all_lists$word) %>%
  #filter(is.element(word, unique(all_lists$word))) %>%
  tasa[tasa$word %in% all_lists$word, ] %>% 
  arrange(-desc(consistency)) %>% 
  mutate(consistency_rank = seq_len(n())) %>%
  arrange(desc(tasa_freq)) %>% 
  mutate(tasa_rank = seq_len(n())) %>%
  select(tasa_rank, consistency_rank, word)

middle_x = middle_y = nrow(rank_inconsistency_rank_frequency)/2

lower_left_quadrant_words = rank_inconsistency_rank_frequency %>%
  filter(tasa_rank < middle_x & consistency_rank < middle_y) %>%
  pull(word)


```

# Question 3: Frequency and Consistency
We have seen that resources vary in terms of the words are selected for instruction, and that this variability exists at a deeper level than the words themselves (i.e., the properties of the words are an additional source of variability). But can words be selected for instruction in an empirical way, based on observable properties? Our study here has been focused on a number of properties that are known to matter for learning, but here we consider two more closely: *frequency* and *consistency*.

### The Prevailing Approach: Weight on Frequency and/or Consistency
The prevailing educational approach involves using words that are common, and those that exhibit atypical structure, or some combination therein. We've seen that some approaches load more heavily on one than the other. Wonders, for example, contains many very frequent words, but is close to the average for consistency are across the six lists. The Dolch and Fry words were selected because they are common, and they tend to contain words that are about average in consistency to the TASA words (though the lists do contain atypical words like "one", "two", and "the" because they are frequent). Fundations and Kilpatrick, on the other hand, contain words that are the most inconsistent on average, but tend to be less frequent. Regardless of how any given set is described, approaches differentially weight these two factors. Of course, each of these dimensions can be measured, and therefore we can derive sets that are load across both variables simultaneously.

### An Empirical Approach
A simple approach that lends itself well to visual interpretation is placing a set of words in the two-dimensional space defined by the two variables. Within the two-dimensional space words can be measured based on their position relative to some reference point (i.e., point 0, 0 - the origin). In a two-dimensional space where the x-axis is ordered in terms of ranked frequency (further right indicates less frequent) and the y-axis is ordered in terms of rank inconsistency (further up the axis indicates more typical structure) the origin (lowest left point of the plot) represents the most (jointly) frequent and inconsistent (i.e., atypical) word in terms of its print-speech property. This method is a straightforward one because it affords a simple visual interpretation: words that fall towards the lower left portion (e.g., quadrant) of the plot are common and contain idiosyncratic structure, and likely to be useful for "sight word"-type instruction[^frequency_and_consistency-1].

[^frequency_and_consistency-1]: Note that identifying a single quadrant is somewhat arbitrary. Other regions closer to the origin of the two-dimensional space could be considered, or some quantity of words could be identified based on the amount of time available for instruction.

In order to simplify the interpretation the words' position within the two-dimensional space, we can think of the four resulting quadrants each defining its own category withing the frequency-inconsistency distribution (starting from bottom left): inconsistent and frequent (the target region for instructional words), consistent and frequent, consistent and infrequent, and inconsistent and infrequent. Quadrants are used for this purpose because they offer a simple way of delineating a two-dimensional space. A smaller space is considered later. Figure 6, Panel A shows this organization, where Panel B illuminates points (in black) in the target quadrant for special words: those that are both the most frequent and least consistent. The resulting distribution in the lower left quadrant consists of `r length(lower_left_quadrant_words)` total words[^frequency_and_consistency-2].

[^frequency_and_consistency-2]: Note that this is simply the set that is lower in rank than the mean rank for inconsistency and frequency.

```{r FrequencyConsistencyPlot1, fig.cap="The figure (Panel A) shows a key for interpreting the four quadrants across the two-dimensional space defined by word frequency (x-axis) and spelling-sound inconsistency (y-axis), and a depiction of the target quadrant (lower-left; words that are inconsistent and frequent) shown within the pink shaded box in Panel B. All words from the TASA set used as a normative sample are shown as hexbins (darker hex indicates more words populate that space). Frequency and inconsistency are shown as rank versions of those quantitative variables. The semilinear clustering in the upper portion of Panel B results due to many words having a consistency value of one (i.e., inconsistency of zero), where points simply become ordered randomly by the rank computation. Quadrants are defined by the median value for each variable/axis (a rank of 9,735).", fig.width=7}



plot_1 = rank_inconsistency_rank_frequency  %>% 
  ggplot(aes(tasa_rank, consistency_rank)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  theme_apa() +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Key")+ 
  #annotate("text", x = 4300, y = 15200, label = "Consistent & Frequent", size = 2.5) +
  #annotate("text", x = 4300, y = 5000, label = "Inconsistent & Frequent", size = 2.5) +
  #annotate("text", x = 4300, y = 4000, label = "(target region)", size = 2.5) +
  #annotate("text", x = 15000, y = 4400, label = "Inconsistent & Infrequent", size = 2.5) +
  #annotate("text", x = 15000, y = 15200, label = "Consistent & Infrequent", size = 2.5) + # +
  annotate("text", x = 200, y = 700, label = "Consistent & Frequent", size = 2.5) +
  annotate("text", x = 200, y = 190, label = "Inconsistent & Frequent", size = 2.5) +
  annotate("text", x = 200, y = 220, label = "(target region)", size = 2.5) +
  annotate("text", x = 700, y = 700, label = "Inconsistent & Infrequent", size = 2.5) +
  annotate("text", x = 700, y = 210, label = "Consistent & Infrequent", size = 2.5) + # +
  scale_y_continuous(limits = c(0, 888)) +
  scale_x_continuous(limits = c(0, 888)) +
  theme(text = element_text(family = "Times"),
        axis.text.x = element_text(angle = 45, hjust = 1))



plot_2 = rank_inconsistency_rank_frequency %>% 
  mutate(lower_left = case_when(consistency_rank < middle_y  & tasa_rank < middle_y ~ T,
                                TRUE ~ F)) %>% 
  ggplot(aes(tasa_rank, consistency_rank)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  geom_hex(color = "black") +
  annotate("rect", xmin = c(-10, -10), xmax = c(450, -10), ymin = c(-10, -10), ymax = c(450, 450), color = NA, fill = "pink", alpha = .3) +
  labs(x = "Rank Frequency", title = "Target Quadrant (in shaded lower left)") +
  scale_fill_gradient(low = "grey92", high = "black") +
  theme_apa() +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        text = element_text(family = "Times"),
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_grid(plot_1, plot_2, labels = c("A", "B"))
```

## Rankings across Inconsistency & Frequency

A simple rank measure of these two dimensions is the distance of the word from the origin (point 0, 0 in the plot). For this purpose, we can use Euclidean distance (for clarity the equation is provided), where each word's coordinate is its rank frequency ($f$) and inconsistency ($c$), respectively, where the distance between the two points is denoted as $d(c, f)$. $n$ is defined as 2 given that we are working in 2 dimensions, where $i$ is summed over the two dimensions and then a square root is applied.

$$
d\left( c,f\right)   = \sqrt {\sum _{i=1}^{n=2}  \left( c_{i}-f_{i}\right)^2 }
$$

Deriving the set in this way is useful because it is simple, empirically motivated, and permits a straightforward interpretation with respect to the multivariate distribution from which the measure is calculated. Table X shows the top 20 words based on this ranking (a full list of rankings can be found in the supplementary materials online).


```{r EuclideanDistances1}
rank_inconsistency_rank_frequency$distance = NA

for (i in seq(nrow(rank_inconsistency_rank_frequency))){
  ic_ = rank_inconsistency_rank_frequency$consistency_rank[i]
  f = rank_inconsistency_rank_frequency$tasa_rank[i]
  
  distance = l2(c(0, 0), c(ic_, f))
  
  rank_inconsistency_rank_frequency$distance[i] = distance
  
  
}

rank_inconsistency_rank_frequency = rank_inconsistency_rank_frequency %>% 
  arrange(-desc(distance)) %>% 
  mutate(rank = seq_len(n())) %>% 
  select(rank, word, distance, tasa_rank, consistency_rank)


rank_inconsistency_rank_frequency %>% 
  filter(rank <= 50) %>% 
  mutate(word = case_when(word == "i'm" ~ "I'm",
                          word == "i'll" ~ "I'll",
                          TRUE ~ word)) %>% 
  rename(Rank = rank, Word = word, `Distance (L2)` = distance,
         `Inconsistency (Rank)` = consistency_rank, `Frequency (Rank)` = tasa_rank) %>% 
  apa_table(caption = "Top Ranked Words across Sources based Jointly on Inconsistency and Frequency", note = "Rankings derived from Euclidean distance of inconsistency and frequency ranking against the origin (0, 0). The calculated distance is also provided.")

```

Notice how different this method is from one that selects for frequency or inconsistency alone. Words with relatively high rankings on frequency or inconsistency are positioned at the top of this distribution. This can be seen even in the table showing the top ranking words (for example, "who" or "have" by frequency, or "one" or "of" by inconsistency). Importantly, an empirical method along these lines allows us to derive a measure or instructional importance based on concrete principles, as opposed to other methods that select for structural variables in arbitrary ways, which often rely on unvalidated educational theories, subjective measures of importance or both.

## Results
### Where Programs Fall Overall in the Two Dimensions
We can compare different sets of words as to where in this two-dimensional space they fall, expressing how well each program covers these words as a proportion, shown in Table X.

```{r CoverageProgramsLowerQuadrantTable}
table_data = tibble(Source = names(all_lists_),
                    `Proportion of Quadrant` = NA,
                    `Prop. of Resource Total` = NA)

for (i in seq(nrow(table_data))){
  
  source = table_data$Source[i]
  source_words = unlist(all_lists_[source])
  source_subset_in_quadrant = intersect(source_words, lower_left_quadrant_words)
  table_data$`Proportion of Quadrant`[i] =  length(source_subset_in_quadrant)/length(lower_left_quadrant_words)
  table_data$`Prop. of Source Total`[i] = length(source_subset_in_quadrant)/length(source_words)
  }
  
table_data %>% 
  mutate(Source = str_replace(Source, "_", " & ")) %>% 
  arrange(-desc(Source)) %>%
  apa_table(caption = "Coverage of Programs for Words in Lower Left Quadrant",
            note = "Words in lower left quadrant are those that fall below the mean on both rank inconsistency and rank frequency. The total number of words in that quadrant is 178.")
  


```

The critical observations here are (1) the sources vary in terms of their coverage of the words in the lower left quadrant with all covering a some portion of those `r length(lower_left_quadrant_words)` words, and (2) this coverage represents only portion of the total number of words in each program (rightmost column of Table XX). This can be seen by plotting the two-dimensional distribution for each program as well, shown in Figure XX. The tension between proportion of coverage of the target words and those additional words that programs include outside this distribution can be seen most clearly in the Wonders pane. This program has approximately half coverage (Table XX) of the target quadrant. However, the distribution of words in that program spreads robustly across all quadrants of the figure, including many words in the "infrequent and inconsistent" category.

```{r FrequencyConsistencyPlot2, fig.width=7, fig.cap="Words (shown in hexbins, smoothing across proximal points) for all resources are shown in two-dimensional space defined by rank frequency and rank inconsistency. Darker hexbins indicate more points. The dimensions of the space are defined by all words in the TASA sample (K-3). Each plot corresponds to a program, where that program's words are plotted as black points against the words in all other programs (grey points)."}




x_limit = max(rank_inconsistency_rank_frequency$tasa_rank)
y_limit = max(rank_inconsistency_rank_frequency$consistency_rank)


plot_dolch = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Dolch ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Dolch") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))

plot_fp = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Fountas_Pinnell ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Fountas & Pinnell") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_fry = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Fry ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Fry") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_fundations = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Fundations ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Fundations") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_kilpatrick = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Kilpatrick ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Kilpatrick") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_wonders = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Wonders ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Wonders") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_grid(plot_dolch, plot_fp, plot_fry, plot_fundations, plot_kilpatrick, plot_wonders)

```

All programs have some amount of coverage in all four quadrants, though a few more specific trends can be seen in the graphical depiction. Wonders, Fountas & Pinnell, Fry, and Dolch contain the most dense clustering (darkest hex portions) on the left side of the space, including in the target region if we are identifying frequent and inconsistent words specifically. They contain 89% (Wonders), 67% (F&P), 66% (Fry), and 58% (Dolch) of the lower left quadrant words. These four programs also contain the most densely populated upper left quadrant (frequent and consistent), shown with the darker hex regions in that portion of each graph. Fundations is more evenly spread out across both dimensions, with less clustering in general (a property made possible given that the program has a smaller set of words). The Kilpatrick program loads most heavily towards the righthand portion of the space, with dense clustering in both the "infrequent and inconsistent" and "infrequent and consistent" quadrants. This accords with other analyses here that demonstrate its outlier status among the programs.

### Top Words from Origin by Distance
By measuring each word from point 0, 0 and rank ordering based on distance, we achieve a distribution of words that is arranged in similarity from the point representing the extreme of rank frequency and rank inconsistency (i.e., the most frequent, inconsistent position in the 2-dimensional space). Table XX showed the top 50 words in the distribution calculated in this way, and Figure XX visualizes these 50 words in the two dimensional space, where the origin is located in the bottom right corner.


```{r fig.cap='The plot shows the top 50 words ranked jointly in terms of frequency and inconsistency using Euclidean distance to calculate the rankings. The word closest to the origin is "and", representing the word that is most frequent and atypical when both properties are considered. Words that fall toward the diagonal have similar values in the rank of each variable. Note that the word "and" is measured as a low consistency word in the Chee et al. (2020) data despite it having neighbors with similar pronunciations, as in "hand" and "band".'}

rank_inconsistency_rank_frequency %>% 
  slice_min(n = 50, order_by = distance) %>% 
  ggplot(aes(tasa_rank, consistency_rank, label = word)) +
  geom_abline(intercept = 0, slope = 1, color = "grey45", linetype = "dashed") +
  geom_label_repel(max.overlaps = 50) +
  theme_apa() +
  labs(x = "Rank Frequency", y = "Rank Inconsistency")

```

# Discussion
Educational approaches to the instruction of special words tend to focus on words based on their joint properties of frequency and consistency, though instructional schemes for early print vocabulary vary (see General Discussion for more on this point). Traditionally, lists focus at least on the identification of frequent words, given that they are most likely to be useful to the developing child because they are most likely to be encountered in print. The relevance of word consistency is more variable, depending on the specific goals and theoretical framing of the instruction. The earliest "sight word" list [@Dolch1937] and its successors [@Fry1957; @Fry1980] focused on word frequency and the contextual diversity of the words selected (Dolch precluded nouns because of how closely they tend to be tied to the context in which they are used; "...because each noun is tied to special subject matter", p. 459). Modern extensions of the "sight word" idea consider a second property, _consistency_, on the assumption that words that are common and have atypical structure (i.e., those that are inconsistent) should be taught directly to students and those that contain structure that is easily explained by phonic rules will handle the groups of words not taught during this form of instruction.

By using established data to identify these properties we have specified sets of words that are jointly defined based on their distributional characteristics of frequency and inconsistency. Words that have a low value of the joint rank of both represent possible savings in instruction because they are likely to be experienced in print and will have structure that isn't easily generalized from other words that contain similar letters. In the demonstration here the point in the 2-dimensional space representing the extreme on rank frequency and rank inconsistency, where the extreme corresponds to the most frequent and most inconsistent word (the origin of the space), serves as a reasonable starting point if our goal is to teach children to read words that are very common and also contain idiosyncratic structure.

```{r}
rank_inconsistency_rank_frequency %>% 
  left_join(read_csv('../words/elp/elp_full_5.27.16.csv') %>% 
    select(word = Word, rt = I_NMG_Mean_RT, acc = I_NMG_Mean_Accuracy)) %>% 
  #summarise(cor(rt, distance, use = "pairwise.complete.obs", method = "spearman"))
  ggplot(aes(distance, rt, label = word)) +
  geom_point() +
  geom_smooth(method = "loess") +
  geom_text()

```

The six programs studied here differ in the extent to which they jointly consider these two variables in selecting instructional words. This can be seen in terms of examining the lower left quadrant of the bivariate distribution of rank frequency and rank inconsistency, as well as examining the top 20, 50, 100, and 250 words in the joint rank distribution (defined by the Euclidean calculation from origin).
