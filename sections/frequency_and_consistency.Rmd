```{r WordsInLowerLeft}

rank_inconsistency_rank_frequency = #tasa %>% 
  #filter(word %in% all_lists$word) %>%
  #filter(is.element(word, unique(all_lists$word))) %>%
  tasa[tasa$word %in% all_lists$word, ] %>% 
  arrange(-desc(consistency)) %>% 
  mutate(consistency_rank = seq_len(n())) %>%
  arrange(desc(tasa_freq)) %>% 
  mutate(tasa_rank = seq_len(n())) %>%
  select(tasa_rank, consistency_rank, word)

middle_x = middle_y = nrow(rank_inconsistency_rank_frequency)/2

lower_left_quadrant_words = rank_inconsistency_rank_frequency %>%
  filter(tasa_rank < middle_x & consistency_rank < middle_y) %>%
  pull(word)


```

# Question 3: Frequency and Consistency
We have seen that resources vary in terms of the words selected for instruction, and that this variability exists at a deeper level than the words themselves (i.e., the properties of the words are an additional source of variability). But can words be selected for instruction in an empirical way, based on observable properties? Our study here has been focused on a number of properties that are known to matter for learning, but here we consider two more closely: *frequency* and *consistency*.

### The Prevailing Approach: Weight on Frequency and/or Consistency
The prevailing educational approach involves using words that are common, and those that exhibit atypical structure, or some combination therein. Some approaches load more heavily on one than the other. Wonders, for example, contains many very frequent words, but is close to the average for consistency are across the six lists. The Dolch and Fry words were selected because they are common, and they tend to contain words that are about average in consistency to the TASA words (though the lists do contain atypical words like "one", "two", and "the" because they are frequent). Fundations and Kilpatrick, on the other hand, contain words that are the most inconsistent on average, and tend to be less frequent. Regardless of how any given set is described, approaches differentially weight these two factors. Of course, each of these dimensions can be measured, and therefore we can derive sets that are load across both variables simultaneously.

### An Empirical Approach
A simple approach that lends itself well to visual interpretation is placing a set of words in the two-dimensional space defined by the two variables. Within the two-dimensional space words can be measured based on their position relative to some reference point (i.e., point 0, 0 - the origin). In a two-dimensional space where the x-axis is ordered in terms of ranked frequency (further right indicates less frequent) and the y-axis is ordered in terms of rank inconsistency (further up the axis indicates more typical structure) the origin (lowest left point of the plot) represents the most jointly frequent and inconsistent (i.e., atypical) word in terms of its print-speech properties. This method is a straightforward one because it affords a simple visual interpretation: words that fall towards the lower left portion of the plot are common and contain idiosyncratic structure, and likely to be useful for "sight word"-type instruction[^frequency_and_consistency-1].

[^frequency_and_consistency-1]: Identifying a single quadrant is somewhat arbitrary. Other regions closer to the origin of the two-dimensional space could be considered, or some quantity of words could be identified based on the amount of time available for instruction.

In order to simplify the interpretation the words' position within the two-dimensional space, we can think of the four resulting quadrants each defining its own category within the frequency-inconsistency distribution (starting from bottom left): inconsistent and frequent (the target region for instructional words), consistent and frequent, consistent and infrequent, and inconsistent and infrequent. Quadrants are used for this purpose because they offer a simple way of delineating a two-dimensional space. An alternative based on a joint ranking is considered later. Figure 6, Panel A shows this organization, where Panel B illuminates points (in black) in the target quadrant for special words: those that are both the most frequent and least consistent. The resulting distribution in the lower left quadrant consists of `r length(lower_left_quadrant_words)` total words[^frequency_and_consistency-2].

[^frequency_and_consistency-2]: Note that this is simply the set that is lower in rank than the median rank for inconsistency and frequency.

```{r FrequencyConsistencyPlot1, fig.cap="The figure (Panel A) shows a key for interpreting the four quadrants across the two-dimensional space defined by word frequency (x-axis) and spelling-sound inconsistency (y-axis), and a depiction of the target quadrant (lower-left; words that are inconsistent and frequent) shown within the pink shaded box in Panel B. All words from the six programs used as a normative sample are shown as hexbins (darker hex indicates more words populate that space). Frequency and inconsistency are shown as rank versions of those quantitative variables. The semilinear clustering in the upper portion of Panel B results due to many words having a consistency value of one (i.e., inconsistency of zero), where points become ordered by the rank computation. Quadrants are defined by the median value for each variable/axis (a rank of 444).", fig.width=7}



plot_1 = rank_inconsistency_rank_frequency  %>% 
  ggplot(aes(tasa_rank, consistency_rank)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  theme_apa() +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Key") + 
  annotate("text", x = 200, y = 700, label = "Consistent & Frequent", size = 2.5) +
  annotate("text", x = 200, y = 190, label = "Inconsistent & Frequent", size = 2.5) +
  annotate("text", x = 200, y = 220, label = "(target region)", size = 2.5) +
  annotate("text", x = 700, y = 700, label = "Inconsistent & Infrequent", size = 2.5) +
  annotate("text", x = 700, y = 210, label = "Consistent & Infrequent", size = 2.5) +
  scale_y_continuous(limits = c(0, 888)) +
  scale_x_continuous(limits = c(0, 888)) +
  theme(text = element_text(family = "Times"),
        axis.text.x = element_text(angle = 45, hjust = 1))



plot_2 = rank_inconsistency_rank_frequency %>% 
  mutate(lower_left = case_when(consistency_rank < middle_y  & tasa_rank < middle_y ~ T,
                                TRUE ~ F)) %>% 
  ggplot(aes(tasa_rank, consistency_rank)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  geom_hex(color = "black") +
  annotate("rect", xmin = c(-10, -10), xmax = c(450, -10), ymin = c(-10, -10), ymax = c(450, 450), color = NA, fill = "pink", alpha = .3) +
  labs(x = "Rank Frequency", title = "Target Quadrant (in shaded lower left)") +
  scale_fill_gradient(low = "grey92", high = "black") +
  theme_apa() +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        text = element_text(family = "Times"),
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_grid(plot_1, plot_2, labels = c("A", "B"))


```

## Rankings across Inconsistency & Frequency

A simple rank measure of these two dimensions is the distance of the word from the origin (point 0, 0 in the plot). For this purpose, we can use Euclidean distance (for clarity the equation is provided), where each word's coordinate is its rank frequency ($f$) and inconsistency ($c$), respectively, where the distance between the two points is denoted as $d(c, f)$. $n$ is defined as 2 given that we are working in 2 dimensions, where $i$ is summed over the two dimensions and then a square root is applied.

$$
d\left( c,f\right)   = \sqrt {\sum _{i=1}^{n=2}  \left( c_{i}-f_{i}\right)^2 }
$$

```{r chee_token_consistency_data, include=FALSE}
chee_token_consistency_corr = rank_inconsistency_rank_frequency %>% 
        left_join(readxl::read_xlsx('../words/consistency/13428_2020_1391_MOESM1_ESM.xlsx', sheet = 2) %>% 
          filter(word %in% rank_inconsistency_rank_frequency$word) %>% 
          mutate(consistency = case_when(n_syll == 1 ~ ff_1_r,
                                         n_syll > 1 ~ ff_all_r)) %>% 
          select(word, consistency) %>% 
          arrange(-desc(consistency)) %>% 
          mutate(token_consistency_rank = seq_len(n())) %>% 
          select(word, token_consistency_rank)) %>% 
          summarise(cor(distance, token_consistency_rank, use = "pairwise.complete.obs"))
```

Deriving the set in this way is useful because it is simple, empirically motivated, and permits a straightforward visual understanding. It is conceptually similar to calculating consistency over word tokens rather than types given that token-based estimation takes into consideration the frequency of the pattern defining the consistency metric (here we use feedforward average body-rime consistency from @Chee2020). The measure calculated using distance, rather, is conditioned on the frequency of the _word_ rather than the sublexical pattern defining consistency. This has the effect of privileging the word's frequency beyond the frequency of the body-rime unit.  The correlation between the distance-based measure with the rank ordered token-based consistency^[The token-based measure we use here is identical to the type-based measure reported everywhere else here except that it is taken from the Chee et al. (2020) "token" data sheet rather than than their "type" data sheet.] from @Chee2020 is _r_ = `r round(chee_token_consistency_corr$`cor(distance, token_consistency_rank, use = "pairwise.complete.obs")`, digits = 2)`.  Table XX shows the top 50 words based on the ranking by distance (a full list of rankings can be found in the supplementary materials online).


```{r EuclideanDistances1}
rank_inconsistency_rank_frequency$distance = NA

for (i in seq(nrow(rank_inconsistency_rank_frequency))){
  ic_ = rank_inconsistency_rank_frequency$consistency_rank[i]
  f = rank_inconsistency_rank_frequency$tasa_rank[i]
  
  distance = l2(c(0, 0), c(ic_, f))
  
  rank_inconsistency_rank_frequency$distance[i] = distance
  
  
}

rank_inconsistency_rank_frequency = rank_inconsistency_rank_frequency %>% 
  arrange(-desc(distance)) %>% 
  mutate(rank = seq_len(n())) %>% 
  select(rank, word, distance, tasa_rank, consistency_rank)


rank_inconsistency_rank_frequency %>% 
  filter(rank <= 50) %>%
  mutate(word = case_when(word == "i" ~ "I",
                          word == "i'm" ~ "I'm",
                          word == "i'll" ~ "I'll",
                          TRUE ~ word)) %>% 
  rename(Rank = rank, Word = word, `Distance (L2)` = distance,
         `Inconsistency (Rank)` = consistency_rank, `Frequency (Rank)` = tasa_rank) %>% 
  apa_table(caption = "Top Ranked Words across Sources based Jointly on Inconsistency and Frequency", note = "Rankings derived from Euclidean distance of inconsistency and frequency ranking against the origin (0, 0). The calculated distance is also provided.")

```

Notice how different this method is from one that selects for frequency or inconsistency alone. Words with relatively high rankings on frequency or inconsistency are positioned at the top of this distribution. This can be seen even in the table showing the top ranking words (for example, "who" or "have" by frequency, or "one" or "of" by inconsistency). Importantly, an empirical method along these lines allows us to derive a measure or instructional importance based on concrete principles, as opposed to other methods that select for structural variables in arbitrary ways, which often rely on unvalidated educational theories, subjective measures of importance, or both.

## Results
### Where Programs Fall Overall in the Two Dimensions
We can compare different sets of words as to where in this two-dimensional space they fall, expressing how well each program covers these words as a proportion, shown in Table XX.

```{r CoverageProgramsLowerQuadrantTable}
table_data = tibble(Source = names(all_lists_),
                    `Proportion of Quadrant` = NA,
                    `Prop. of Resource Total` = NA)

for (i in seq(nrow(table_data))){
  
  source = table_data$Source[i]
  source_words = unlist(all_lists_[source])
  source_subset_in_quadrant = intersect(source_words, lower_left_quadrant_words)
  table_data$`Proportion of Quadrant`[i] =  length(source_subset_in_quadrant)/length(lower_left_quadrant_words)
  table_data$`Prop. of Resource Total`[i] = length(source_subset_in_quadrant)/length(source_words)
  }
  
table_data %>% 
  mutate(Source = str_replace(Source, "_", " & ")) %>% 
  arrange(-desc(Source)) %>%
  apa_table(caption = "Coverage of Programs for Words in Lower Left Quadrant",
            note = "Words in lower left quadrant are those that fall below the mean on both rank inconsistency and rank frequency. The total number of words in that quadrant is 187.")
  
```

The critical observations here are (1) the sources vary in terms of their coverage of the words in the lower left quadrant with all covering a some portion of those `r length(lower_left_quadrant_words)` words, and (2) this coverage represents only portion of the total number of words in each program (rightmost column of Table XX). This can be seen by plotting the two-dimensional distribution for each program as well, shown in Figure XX. The tension between proportion of coverage of the target words and those additional words that programs include outside this distribution can be seen most clearly in the Wonders pane. This program has approximately half coverage (Table XX) of the target quadrant. However, the distribution of words in that program spreads robustly across all quadrants of the figure, including many words in the "infrequent and inconsistent" category.

```{r FrequencyConsistencyPlot2, fig.width=7, fig.cap="Words (shown in hexbins, smoothing across proximal points) for all resources are shown in two-dimensional space defined by rank frequency and rank inconsistency. Darker hexbins indicate more points. The dimensions of the space are defined by all words in the TASA sample (K-3). Each plot corresponds to a program."}




x_limit = max(rank_inconsistency_rank_frequency$tasa_rank)
y_limit = max(rank_inconsistency_rank_frequency$consistency_rank)


plot_dolch = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Dolch ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Dolch") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))

plot_fp = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Fountas_Pinnell ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Fountas & Pinnell") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_fry = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Fry ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Fry") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_fundations = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Fundations ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Fundations") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_kilpatrick = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Kilpatrick ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Kilpatrick") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_wonders = rank_inconsistency_rank_frequency %>% 
  mutate(in_program = case_when(word %in% all_lists_$Wonders ~ TRUE,
                                TRUE ~ FALSE)) %>% 
  filter(in_program == TRUE) %>% 
  ggplot(aes(tasa_rank, consistency_rank, color = in_program)) +
  geom_hline(aes(yintercept = middle_y)) +
  geom_vline(aes(xintercept = middle_x)) +
  #geom_point(size = .05) +
  geom_hex(linewidth = .1) +
  scale_fill_gradient(low = "grey99", high = "black") +
  labs(x = "Rank Frequency", y = "Rank Inconsistency", title = "Wonders") +
  scale_color_manual(values = c("black")) +
  theme_apa() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = .5),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank()) +
  xlim(c(0, x_limit)) +
  ylim(c(0, y_limit))


plot_grid(plot_dolch, plot_fp, plot_fry, plot_fundations, plot_kilpatrick, plot_wonders)

```

All programs have some amount of coverage in all four quadrants, though a few more specific trends can be seen in the graphical depiction. Wonders, Fountas & Pinnell, Fry, and Dolch contain the most dense clustering (darkest hex portions) on the left side of the space, including in the target region if we are identifying frequent and inconsistent words specifically. They contain 89% (Wonders), 67% (F&P), 66% (Fry), and 58% (Dolch) of the lower left quadrant words. These four programs also contain the most densely populated upper left quadrant (frequent and consistent), shown with the darker hex regions in that portion of each graph. Fundations is more evenly spread out across both dimensions, with less clustering in general (a property made possible given that the program has a smaller set of words). The Kilpatrick program loads most heavily towards the righthand portion of the space, with dense clustering in both the "infrequent and inconsistent" and "infrequent and consistent" quadrants. This accords with other analyses here that demonstrate its outlier status among the programs.

### Top Words from Origin by Distance
By measuring each word from point 0, 0 and rank ordering based on distance, we achieve a distribution of words that is arranged in similarity from the point representing the extreme of rank frequency and rank inconsistency (i.e., the most frequent, inconsistent position in the 2-dimensional space). Table XX showed the top 50 words in the distribution calculated in this way, and Figure XX visualizes these 50 words in the two dimensional space, where the origin is located in the bottom right corner.

```{r fig.cap='The plot shows the top 50 words ranked jointly in terms of frequency and inconsistency using Euclidean distance to calculate the rankings. The word closest to the origin is "and", representing the word that is most frequent and atypical when both properties are considered. Words that fall toward the diagonal have similar values in the rank of each variable. Note that the word "and" is measured as a low consistency word in the Chee et al. (2020) data despite it having neighbors with similar pronunciations, as in "hand" and "band".'}

rank_inconsistency_rank_frequency %>% 
  mutate(word = case_when(word == "i" ~ "I",
                          word == "i'll" ~ "I'll",
                          TRUE ~ word)) %>% 
  slice_min(n = 50, order_by = distance) %>% 
  ggplot(aes(tasa_rank, consistency_rank, label = word)) +
  geom_abline(intercept = 0, slope = 1, color = "grey45", linetype = "dashed") +
  geom_label_repel(max.overlaps = 50) +
  theme_apa() +
  labs(x = "Rank Frequency", y = "Rank Inconsistency")

```

The quadrant-based results limit the number of words to those that fall on the median of each axis. Discretizing the words into four groups (quadrants) aids a visual interpretation, but is somewhat arbitrary. Alternatively, using the distance from origin, we can examine how each program falls within sets of words defined by this distance, using several different quantities. Here we consider $n \in \{20, 50, 100, 250\}$.


```{r top_n_words_frequency_inconsistency, fig.cap="The proportion of n furthest words from origin in the two dimensional space organized by inconsistency and frequency are shows for values of n = 20, 50, 100, and 250. Panels are arranged left to right from greatest to least overall proportion covered (averaging across proportions of all values of n for a given program).", fig.width=7}



programs_by_rank_inconsistency_rank_frequency %>% 
  mutate(Source = case_when(source == 'Dolch' ~ 'Dolch',
                            source == 'Fry' ~ 'Fry',
                            source == 'Fundations' ~ 'Fundations',
                            source == 'Kilpatrick' ~ 'Kilpatrick',
                            source == 'Wonders' ~ 'Wonders',
                            source == 'Fountas_Pinnell' ~ 'F&P')) %>% 
  ggplot(aes(factor(n), Proportion, fill = as.factor(n))) +
  geom_bar(stat = "identity", position = "dodge", color = "black") +
  geom_text(aes(label = sub("^0\\.", ".", round(Proportion, digits = 2))), size = 2.5, vjust = 1.8, alpha = .8) +
  facet_grid(~factor(Source, levels = c("Wonders", "Fry", "Dolch", "F&P", "Fundations", "Kilpatrick"))) +
  theme_apa() +
  labs(x = "N furthest from origin", y = "Proportion covered", fill = "N furthest from origin") +
  theme(text = element_text(family = "Times"),
        axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.position = "none") +
  scale_fill_manual(values = c("burlywood", "gold2", "brown", "darkorchid4"))



```

These results parallel those from the quadrant-based analysis previously, but here we highlight a few important points. Wonders provides the best coverage across all values of _n_, covering all top 20 words by distance, 98% or the top 50 words, 94% of the top 100 words, and 88% of the top 250 words. Both the Fry and Dolch resources have perfect coverage of the top 20 words, and high coverage of the top 50 words (96% and 92% respectively). Fundations and Kilpatrick lists occupy the other end of the extreme, with Kilpatrick ranking last in all proportions across values for n. This is somewhat surprising given the focus of Kilpatrick's resources is on irregular words. However, because the list contains relatively infrequent words, the coverage of the sets of top 20, 50, 100, and 250 words by frequency-inconsistency distance remains quite low (especially relative to the other programs). The Fundations list ranks next-lowest overall. While Fundations contains 95% of the top 20 words defined by this variable, its coverage of the top 50, 100, and 250 words are relatively low compared to the the others.


## Discussion
Educational approaches to the instruction of special words tend to focus on words based on their joint properties of frequency and consistency, though instructional schemes for early print vocabulary vary (see General Discussion for more on this point). Traditionally, resources focus at least on the identification of frequent words, given that they are most likely to be useful to the developing child because they are most likely to be encountered in print. The relevance of word consistency is less settled, depending on the specific goals and theoretical framing of the instruction. The earliest "sight word" list [@Dolch1936] and its successors [@Fry1957; @Fry1980] focused on word frequency and the contextual diversity of the words selected (Dolch precluded nouns because of how closely they tend to be tied to the context in which they are used; "...because each noun is tied to special subject matter", p. 459). Modern extensions of the "sight word" idea consider a second property, _consistency_, on the assumption that words that are common and have atypical structure (i.e., those that are inconsistent) should be taught directly to students and those that contain structure that is easily explained by phonic rules will handle the groups of words not taught during this form of instruction.

By using established data to identify these properties we have specified sets of words that are jointly defined based on their distributional characteristics of frequency and inconsistency. Words that have a low value of the joint rank of both represent possible savings in instruction because they are likely to be experienced in print and will have structure that isn't easily generalized from other words that contain similar letters. In the demonstration here the point in the 2-dimensional space representing the extreme on rank frequency and rank inconsistency, where the extreme corresponds to the most frequent and most inconsistent word (the origin of the space), serves as a reasonable starting point if our goal is to teach children to read words that are very common and also contain idiosyncratic structure.

The six programs studied here differ in the extent to which they jointly consider these two variables in selecting instructional words. This can be seen in terms of examining the lower left quadrant of the bivariate distribution of rank frequency and rank inconsistency, as well as examining the top 20, 50, 100, and 250 words in the joint rank distribution (defined by the Euclidean calculation from origin). Wonders shows strong coverage of the top ranking words the join frequency-inconsistency distribution, as do Fry and Dolch. It is noteworthy that the Fry and Dolch wordlists have high coverage here because they also tend to contain more consistent words on average (e.g, see Figure 7). Relatively low coverage is achieved by Fundations and Kilpatrick, again demonstrating that these two resources differ from the others in terms of the properties of the words they contain. Wonders contains the largest number of words overall, and therefore holds an advantage over the other programs in terms of the likelihood of having strong coverage on these and other variables examined. At the other end of the extreme, Kilpatrick's resource contains a large number of atypical words in terms of spelling-sound structure, but also tends to contain infrequent words. So these results aren't as surprising. Fundations relatively low coverage here stands out, however. Their program described their approach to sight word instruction as focusing on words that are "tricky" due to their spelling-sound structure in addition to their frequency. However, Fundations contains only 45% of the top 250 words when considering frequency and inconsistency ranked jointly (by distance).


