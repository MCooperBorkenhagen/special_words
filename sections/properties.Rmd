# Question 2: The Overlap across Sources in Terms of their Properties

It is possible that the words across resources might be different while the underlying properties of those words might not vary. If this is the case then variability in the words selected for instruction would be less meaningful; Students might have similar language experience in instruction by virtue of the underlying language structures the words convey. In order to examine this possibility we will look at each set of words binned by their overlap among the six instructional resources (the number of resources that the word appeared in) and examine that set of words in terms of their language properties, starting with word frequency.

In order to analyze instructional words with respect to their linguistic properties, a number of datasets were identified and merged with the database of words aggregated for the study. One important issue concerns how instructional words compare to words children and adults tend to encounter in other language experiences - reading related and otherwise. In order to make comparisons along these lines we assembled several relevant benchmark sets using language databases that represent language experiences for different groups and in different modes of communication, both via printed and spoken language. The corpora we selected for this purpose (in terms of examining word frequency) are The Wisconsin Children's Book Corpus [@Lewis2022], the Child Language Data Exchange System [@MacWhinney2000], the Educator's Word Frequency Guide from Touchstone Applied Science Associates [@Zeno1995], and the Corpus of Contemporary American English [@Davies2010].

The Wisconsin Children's Book Corpus (WCBC) contains texts commonly read by and to pre-readers (up to the age of six). Data from the Child Language Data Exchange System (CHILDES) represents language from children's spoken language environment (the data used here are from transcripts for children in the US up to the age of six). The Educator's Word Frequency Guide (from Touchstone Applied Science Associates; TASA) captures information about words in literature children are commonly exposed to in grade school, and The Corpus of Contemporary American English is a large corpus of texts aimed at adult readers (COCA; this subset is adult fiction).

```{r frequencyCorpusTable, echo=FALSE, warning=FALSE}
frequency_by_corpus(all_lists) %>% 
  rbind(coca %>% 
              summarise(M = mean(freq, na.rm = T),
                        SD = sd(freq, na.rm = T),
                        max = max(freq)) %>% 
              mutate(Corpus = "COCA")) %>%  # add COCA because it was not included in all_lists
  apa_table(caption = "Descriptive Characteristics for Word Frequencies using for All Words in the Database", note = "Values provided as raw frequencies. Min values are always 1 and have been excluded. WCBC = Wisconsin Childrens Book Corpus; TASA = Frequencies from the Educator's Word Frequency Guide by Touchstone Applied Science Associates; CHILDES = The Child Language Data Exchange System; COCA = The Corpus of Contemporary American English", escape = FALSE)

```

## The Properties of Words across Sources

First we will look at the frequency of words in each instructional resource and their relative distribution across them. Then, we will move on to other word properties.

## Frequency

```{r frequencyModels, include=FALSE}

tmp = Zs %>% 
  filter(var == 'wcbc_rank')
  

z_tests = split(tmp$Z, tmp$source) %>% 
  map(t.test)

z_tests_to_table(z_tests) %>% 
  select(-CI_low, -CI_hi, -p_derived) %>% 
  apa_table(escape = FALSE)


```

All the frequency data in this section are analyzed using rank frequencies, where low values indicate high frequencies. Rank frequencies exhibit a uniform distribution (one word per rank), and this type of frequency measure avoids the skew found in raw frequencies. Meaningful variation exists in the portion of the distribution where there are no ties (i.e., the highest frequency words/ those with the lowest rank frequency). In order to facilitate comparison across sources and relative to a normative distribution (i.e., a distribution from a reference corpus), rank frequencies were standardized. We first present the rank frequency data for all four corpora for reference and then consider TASA frequency data more in depth because the trends across all corpora are similar.

All words from a given corpus were standardized for rank frequency by taking the rank frequency value, subtracting the mean for that corpus and dividing by the standard deviation of that corpus. This results in a distribution of rank frequency for each word in a given corpus (e.g., the rank frequency within the TASA corpus) set in relation to the variability of rank frequency in that corpus. Once achieved, measures of rank frequency within each instructional source were calculated using the standardized rank frequency from each corpus' distribution just described.

This allows comparisons on several levels. First, the distribution of standardized rank frequency for a given source gives a measure of how different that distribution is for the normative distribution. Additionally, the distribution for each resource can be compared to the others given that they've been put on a common scale. The resulting interpretation of the mean of rank frequency for a source is the difference in standard deviation units (of rank) between that source and the corpus from which standardized values have been calculated.

This is useful when one considers the characteristics of a given text corpus from which the frequency distribution is constructed (child-directed text, child-directed speech, adult-directed text, etc.). A table of descriptive statistics across corpora and sources are shown in Table XX.

```{r descriptivesFreqSource, echo=FALSE, warning=FALSE, message=FALSE}
# old code left here for supplement which should contain values across several corpora

sources = desc_by_var(Zs, 'tasa_rank', combine_ = T)$source
#wcbc_ = desc_by_var(Zs, 'wcbc_rank', combine_ = T)$var
tasa = desc_by_var(Zs, 'tasa_rank', combine_ = T)$var
#coca = desc_by_var(Zs, 'coca_rank', combine_ = T)$var
#childes = desc_by_var(Zs, 'childes_rank', combine_ = T)$var

tasa_2 = all_lists %>% 
  arrange(desc(tasa_freq)) %>% 
  mutate(rank = seq_len(n())) %>% 
  group_by(source) %>% 
  summarise(rank_mean = round(mean(rank, na.rm = T)),
            rank_sd = round(sd(rank, na.rm = T)),
            raw_mean = round(mean(tasa_freq, na.rm = T)),
            raw_sd = round(sd(tasa_freq, na.rm = T))) %>% 
  mutate(Rank = str_c(rank_mean, " (", rank_sd, ")"),
         Raw = str_c(raw_mean, " (", raw_sd, ")")) %>% 
  mutate(Source = case_when(source == 'dolch' ~ 'Dolch',
                            source == 'fountas_pinnell' ~ 'Fountas & Pinnell',
                            source == 'fry' ~ 'Fry',
                            source == 'fundations' ~ 'Fundations',
                            source == 'kilpatrick' ~ 'Kilpatrick',
                            source == 'wonders' ~ 'Wonders',
                            TRUE ~ NA
                              )) %>% 
  select(Source, Rank, Raw)


data.frame(cbind(sources, tasa)) %>% 
  rename(Source = sources,
         `Rank (Z)` = tasa,
         ) %>% 
  left_join(tasa_2) %>%
  select(Source, Rank, everything()) %>% 
  apa_table(caption = "Descriptive Statistics of Rank Frequency by Resource for TASA Corpus", 
            note = 'Means and standard deviations (in parentheses) are shown for rank frequency, standardized rank frequency (Rank Z), and raw frequency values for all words in a given instructional resource for the TASA corpus. Frequencies are standardized based on all words in TASA before subsetting and calculating the mean and spread by instructional source. As a result, the mean rank frequency shown represents how far from the mean of all words in TASA the distribution within an instructional resource is.',
            escape = FALSE)



```

All instructional resources contain words that are, on average, more frequent (lower standardized rank frequency) than the average word from TASA, as indicated by each mean value for standardized rank frequency being negative. This is expected given than all of the resources are supposed to contain common words (based on their own reporting).

Considering TASA data specifically, these comparisons bear out in a statistical model of these distributions against zero (the normative mean, which is always zero due to standardization). Figure 3 shows the results from this statistical test for the six resources in the distribution of frequencies in that corpus. Grey points within each panel are individual observations (words) within each list for a given value for standardized rank frequency. The colored points represent the mean for a source on a given frequency, and the bar around that mean represents the standard error of the point estimate for that source on that frequency. The dotted line represents the mean across all frequencies for that instructional source. This facilitates a direct comparison across instructional sources.

Fry tends to contain the highest frequency words, with the lowest average rank frequency, as well as the least amount of variability. On the other end is Kilpatrick. This set of words is much higher in average rank frequency (i.e., lower in raw frequency) than any of the other resources. The variability in rank frequency for this set is noteworthy as well. Not only are the averages for Kilpatrick much higher in rank frequency than any of the other instructional resources, the spread of frequency values for that source is higher as well. The Kilpatrick distribution includes words _above the mean_ (this fact holds for frequencies drawn from the other corpora as well).


```{r wcbcFreqTtest, echo=FALSE, warning=FALSE, message=FALSE}
z_table_tasa %>% 
  select(-CI_hi, -CI_low, -p_derived, -var) %>% 
  apa_table(caption = "Model Outputs by Source for Statistical Test against Mean Rank Frequency from the Educator's Word Frequency Guide (TASA)", note = "Statistical test used was t.test() from base R (R Core Team, 2021). Rank frequencies were Z transformed prior to test, so resulting coefficients express the difference in standard deviations of rank frequency from TASA between the source mean on rank frequency and the TASA mean (0 due to Z transformation).", escape = FALSE)


```




```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap='In Panel A the raw distribution of words for rank frequency (standardized) from the TASA corpus are shown as points (in grey) with point estimates from the statistical model shown as colored bars. Boxes around the point estimates show standard deviations because standard errors are imperceptibly small. All distributions are significantly different than the corpus mean (0), which is shown as the dashed line at zero. Panel B shows the same plot but zoomed in so that the differences across resources can be seen. This panel shows standard errors as error bars around the point estimates. In both panels the dotted lines show the mean across the six resources.', fig.width=7}

# code derivative of this can be used for the supplement because all corpora are included at top level here

VARS = c('tasa_rank', 'wcbc_rank', 'childes_rank', 'coca_rank')


hlines = nsd %>% 
  filter(var %in% VARS) %>% 
  group_by(var) %>% 
  summarise(varmean = mean(M_, na.rm = T)) %>% 
  mutate(var = case_when(var == 'wcbc_rank' ~ 'WCBC',
                         var == 'tasa_rank' ~ 'TASA',
                         var == 'childes_rank' ~ 'CHILDES',
                         var == 'coca_rank' ~ 'COCA')) %>% 
  filter(var == "TASA")

plot_data = Zs %>% 
  filter(var %in% VARS) %>% 
  select(var, source, M = Z) %>% 
  mutate(var = case_when(var == 'wcbc_rank' ~ 'WCBC',
                         var == 'tasa_rank' ~ 'TASA',
                         var == 'childes_rank' ~ 'CHILDES',
                         var == 'coca_rank' ~ 'COCA')) %>% 
  mutate(source_ = fct_relevel(source, 'Kilpatrick', after = 4))

ci_data = z_table_tasa %>% 
  select(source = Source, ci_ = `95\\% CI`) %>% 
  mutate(ci = str_replace(ci_, "\\[", ""),
         ci = str_replace(ci, "\\]", ""),
         ci_lo = str_split(ci, ",", simplify = T)[,1],
         ci_hi = str_split(ci, ",", simplify = T)[,2],) %>% 
  mutate(ci_lo = as.numeric(ci_lo),
         ci_hi = as.numeric(ci_hi)) %>% 
  select(source, ci_lo, ci_hi)

plot_old = Zs %>% 
  group_by(source, var) %>% 
  summarise(M = mean(Z, na.rm = T)) %>% 
  left_join(nsd) %>% 
  filter(var %in% VARS) %>% 
  mutate(var = case_when(var == 'wcbc_rank' ~ 'WCBC',
                         var == 'tasa_rank' ~ 'TASA',
                         var == 'childes_rank' ~ 'CHILDES',
                         var == 'coca_rank' ~ 'COCA'),
                source = as.factor(source)) %>% 
  mutate(source_ = fct_relevel(source, 'Kilpatrick', after = 4)) %>% 
  filter(var == "TASA") %>% 
  ggplot(aes(source_, M, color = source_)) +
  scale_color_manual(values = COLORS_SOURCE) +
  geom_point(data = plot_data, position = position_jitter(width = .04, height = .01), size = .01, color = 'grey') +
  #geom_point(size = 2) +
  #geom_line(color = 'grey') +
  geom_boxplot() +
  #geom_errorbar(aes(ymin = ci_lo, ymax = ci_hi, color = source), width = .3) +
  ylim(c(-2, .5)) +
  labs(x = 'Source', y = 'Rank frequency (standardized)') +
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') +
  geom_hline(data = hlines, aes(yintercept = varmean), color = 'black', linetype = 'dotted') +
  theme_apa() +
  theme(legend.position = 'none',
        strip.background=element_blank(),
        text = element_text(family = 'Times'))


plot_a = Zs %>% 
  mutate(source_ = fct_relevel(source, 'Kilpatrick', after = 4)) %>% 
  filter(var == "tasa_rank") %>% 
  ggplot(aes(source_, Z, color = source_)) +
  scale_color_manual(values = COLORS_SOURCE) +
geom_point(position = position_jitter(width = .04, height = .01), size = .01, color = 'grey') +
  #geom_point(size = 2) +
  stat_summary(fun.data = "mean_sdl", fun.args = list(mult = 1), geom = "crossbar", width = .2) +
  #geom_errorbar(aes(ymin = ci_lo, ymax = ci_hi, color = source), width = .3) +
  #ylim(c(-2, .5)) +
  labs(x = 'Source', y = 'Rank frequency (standardized)') +
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') +
  geom_hline(data = hlines, aes(yintercept = varmean), color = 'black', linetype = 'dotted') +
  theme_apa() +
  theme(legend.position = 'none',
        strip.background=element_blank(),
        text = element_text(family = 'Times'),
        axis.text.x = element_text(angle = 60, hjust = 1))

plot_b = Zs %>% 
  group_by(source, var) %>% 
  summarise(M = mean(Z, na.rm = T)) %>% 
  left_join(nsd) %>% 
  filter(var %in% VARS) %>% 
  mutate(var = case_when(var == 'wcbc_rank' ~ 'WCBC',
                         var == 'tasa_rank' ~ 'TASA',
                         var == 'childes_rank' ~ 'CHILDES',
                         var == 'coca_rank' ~ 'COCA'),
                source = as.factor(source)) %>% 
  mutate(source_ = fct_relevel(source, 'Kilpatrick', after = 4)) %>% 
  filter(var == "TASA") %>% 
  left_join(ci_data) %>%
  ggplot(aes(source_, M, color = source_)) +
  scale_color_manual(values = COLORS_SOURCE) +
  geom_point(size = 2) +
  geom_line(color = 'grey') +
  geom_errorbar(aes(ymin = ci_lo, ymax = ci_hi, color = source), width = .3) +
  ylim(c(-1.79, -1.05)) +
  labs(x = 'Source', y = 'Rank frequency (standardized)') +
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') +
  geom_hline(data = hlines, aes(yintercept = varmean), color = 'black', linetype = 'dotted') +
  theme_apa() +
  theme(legend.position = 'none',
        strip.background=element_blank(),
        text = element_text(family = 'Times'),
        axis.text.x = element_text(angle = 60, hjust = 1))

plot_grid(plot_a, plot_b, labels = c("A", "B"))
```

### Coverage of Most Frequent Words in Child-directed Print and Speech

We could reasonably expect that these resources draw from the most frequent words in child-directed speech and (especially) print. One way of examining this is calculating the proportion of the most frequent *n* words in child-oriented corpora that a given resource contains. Here we've done this for CHILDES, TASA, and WCBC, each representing different important aspects of experiences of elementary-aged children (spoken language, academic prose, and recreational texts for children). Given that these corpora represent relevant learning environments for an early reader, each instructional set should contain a nontrivial proportion of the most frequent words in the corpus. We've calculated these distributions here (shown in Table XX) using the most frequent 100, 500, and 1000 words.

```{r coverageKidCorpus, echo=FALSE, warning=FALSE, message=FALSE}
ROWS = length(unique(all_lists$source))*length(unique(top_N$corpus))*length(unique(top_N$top_n))
COLS = c("source", "corpus", "top_n", "in_program", "proportion")


coverage = data.frame(matrix(nrow = ROWS, ncol = length(COLS)))
colnames(coverage) = COLS

i = 1

for (source_ in unique(all_lists$source)){
  
  trigger = all_lists %>% 
        filter(source == source_) %>% 
        distinct(word) %>% 
        pull(word)
  
  for (corpus_ in unique(top_N$corpus)){
    for (N in unique(top_N$top_n)){
      
      # get all the words for a given source (program)
      
      
      # get all the words from a given corpus as a specific level of "top N" (i.e. 100, 500, 1000)
      target = top_N %>% 
        filter(corpus == corpus_) %>% 
        filter(top_n == N) %>% 
        pull(word)

      coverage$source[i] = source_
      coverage$corpus[i] = corpus_
      coverage$top_n[i] = as.numeric(N)
      coverage$in_program[i] = length(intersect(target, trigger))
      coverage$proportion[i] = length(intersect(target, trigger))/as.numeric(N)
      
      i = i + 1
      
    }}}


```

The trends across corpora for all six programs follow a common pattern. Programs tend to exhibit more coverage for the most frequent 100 words, less so for the most frequent 500 and even less for the most frequent 1000. This general fact is not surprising. Likewise, the coverage for each program for the three child corpora follow a common trend. Coverage for TASA tends to be greater than WCBC, which tends to be greater than CHILDES.

Wonders contains the most words, with 801, so it is perhaps unsurprising that its coverage is greatest. With this quantity of words, it stands out that it only contains 58%, 65%, and 56% of the most frequent 500 words from the three corpora respectively. That list is the only one with enough to potentially contain 100% of those words (which it does not). By contrast, the Dolch and Fry words obtain comparable coverage with fewer words, each with less than half of that than Wonders. Nonetheless, Wonders, Fry, and Dolch demonstrate very similar levels of coverage. All three have greater than 75% coverage for all three corpora on the top 100 words and around half of the top 500 words. Fountas and Pinnell exhibit slightly lower levels of coverage than those three, with Fundations demonstrating even less coverage than Fountas and Pinnell. The coverage for Kilpatrick is very low, even for the most frequent 100 words. This is fact can be emphasized given that Kilpatrick contains the second most number of words behind Wonders, and over 100 words more than the Dolch list.

```{r coverageKidCorpusTable, echo=FALSE, warning=FALSE, message=FALSE}
coverage %>% 
  select(-in_program) %>% 
  mutate(proportion = round(proportion, digits = 2)) %>% 
  pivot_wider(names_from = top_n, values_from = proportion) %>% 
  mutate(proportion = str_c(`100`, `500`, `1000`, sep = " - ")) %>% 
  select(source, corpus, proportion) %>% 
  pivot_wider(names_from = corpus, values_from = proportion) %>% 
  left_join(coverage %>% 
              group_by(source) %>% 
              summarise(proportion_ = mean(proportion)) %>% 
              arrange(desc(proportion_)) %>% 
              mutate(order = seq_len(n()))) %>% 
  arrange(order) %>% 
  mutate(source = case_when(source == 'dolch' ~ 'Dolch',
                            source == 'fry' ~ 'Fry',
                            source == 'fundations' ~ 'Fundations',
                            source == 'kilpatrick' ~ 'Kilpatrick',
                            source == 'wonders' ~ 'Wonders',
                            source == 'fountas_pinnell' ~ 'F&P')) %>% 
  left_join(all_lists %>% 
              group_by(source) %>% 
              summarise(n_ = n()) %>% 
              mutate(source = case_when(source == 'dolch' ~ 'Dolch',
                            source == 'fry' ~ 'Fry',
                            source == 'fundations' ~ 'Fundations',
                            source == 'kilpatrick' ~ 'Kilpatrick',
                            source == 'wonders' ~ 'Wonders',
                            source == 'fountas_pinnell' ~ 'F&P'))) %>% 
  mutate(source = str_c(source, " (", n_, ")")) %>%             
  select(Resource = source, `WCBC (100-500-1000)` = wcbc, `TASA (100-500-1000)` = tasa, `CHILDES (100-500-1000)` = childes) %>% 
  apa_table(caption = "Proportion Coverage of Top 100, 500, and 1000 Words from WCBC, TASA, and CHILDES for the Six Resources.",
            note = "Number of words in each resource is provided next to the name of the resource for reference.",
            col_spanners = list("Corpus" = c(2, 4)))

```

```{r coverageKidCorpusFigure, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Coverage for each program (panel) for each level of most frequent 100, 500, and 1000 words for three child-directed corpora are shown: CHILDES, TASA, and WCBC. Programs exhibit variable coverage of these words for the three corpora, with Wonders, Fry, and Dolch showing comparable levels of coverage at one extreme, and Kilpatrick at the other.", fig.width=7}

coverage %>%
  filter(corpus == "tasa") %>% 
  mutate(source = case_when(source == 'dolch' ~ 'Dolch',
                            source == 'fry' ~ 'Fry',
                            source == 'fundations' ~ 'Fundations',
                            source == 'kilpatrick' ~ 'Kilpatrick',
                            source == 'wonders' ~ 'Wonders',
                            source == 'fountas_pinnell' ~ 'F&P')) %>% 
  ggplot(aes(as.factor(top_n), proportion, fill = as.factor(top_n))) +
  geom_bar(stat = "identity", position = "dodge", color = "black") +
  facet_grid(~factor(source, levels = c("Wonders", "Fry", "Dolch", "F&P", "Fundations", "Kilpatrick"))) +
  theme_apa() +
  labs(x = "N most frequent", y = "Proportion covered", fill = "N most frequent") +
  theme(text = element_text(family = "Times"),
        axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1),
        legend.position = "none") +
  scale_fill_manual(values = c("burlywood", "gold2", "brown", "darkorchid4"))
```

## Other Word Characteristics across Sources

To facilitate comparison of other word properties (other than frequency), the words from the Wisconsin Children's Book Corpus [@Lewis2022] were used as a benchmark set. All the words from the WCBC (*n* = `r nrow(wcbc)`) were aggregated along with a range of word-level measurements. These include age of acquisition [@Kuperman2012], orthography-to-phonology consistency [@Chee2020], imageability [@Stadthagen2006], number of letters, and number of syllables. In order to present data that captures the normative comparison to WCBC a procedure similar to what was done with word frequencies was used. Measurements for word-level variables were standardized through being mean-centered and put into relation to the standard deviation. This was done based on the distribution of that variable over all words in the WCBC corpus (rather than four different corpora as was done with frequency). This is intended to capture the variability on a given variable relative to the distribution in the benchmark (WCBC-based) sample.

Table XX provides the means and standard deviations for sources across all word variables. Figure 5 shows these values for a given variable for each instructional source (colored point in each panel). Colors are conditioned on the five different variables of interest within each panel and the dotted line refers to the average across points, here representing the average across all variables. This way of presenting these data facilitates interpretation given that all the points for all the variables fall on the negative side of the distribution for all sources; all points are below the mean on the standardized scale for all words in WCBC. The dashed line again shows the mean for all variables in all words in WCBC (zero due to standardization).

```{r desc_other, echo=FALSE, warning=FALSE, message=FALSE}

VARS = c('aoa', 'consistency', 'letters', 'syllables', 'imageability')

sources = desc_by_var(Zs, VARS[1], combine_ = T)$source
aoa = desc_by_var(Zs, VARS[1], combine_ = T)$var
consistency = desc_by_var(Zs, VARS[2], combine_ = T)$var
letters = desc_by_var(Zs, VARS[3], combine_ = T)$var
syllables = desc_by_var(Zs, VARS[4], combine_ = T)$var
imageability = desc_by_var(Zs, VARS[5], combine_ = T)$var


data.frame(cbind(sources, aoa, consistency, letters, syllables, imageability)) %>%
  rename(Source = sources,
         AoA = aoa,
         Consistency = consistency,
         Letters = letters,
         Syllables = syllables,
         Imageability = imageability) %>% 
  apa_table(caption = 'Descriptive Statistics of Other Word Variables for each Source across WCBC', 
            note = "Means and standard deviations (in parentheses) are shown over Z transformed rank frequency values for all words in a given instructional source. Standardization happens relative to the Wisconsin Children's Book Corpus. Therefore the mean value for a source for a given variable indicates the average distance from the overall mean within WCBC.", escape = FALSE)

```

The resources always land below the mean on the standardized rank scale of each variable (below the dashed line). All resources contain words that are learned earlier than the average word in WCBC (lower in rank AoA), are less consistent (lower rank consistency), are less imageable (lower rank imageability), have fewer letters (lower rank letters), and fewer syllables (lower rank syllables). Additionally, each resource rarely falls on the mean for all resources (i.e., has a bar that falls on the dotted line), which indicates noteworthy variability across resources.

Fundations, Kilpatrick, and Dolch are associated with more extreme positions, in general, across each of the variables - though in different directions. Fundations favors words with lower age-of-acquisition, lower consistency, lower imageability, and more syllables than the rest. Kilpatrick tends towards words with higher AoA (learned later) and more letters. Dolch tends towards words that are more consistent and more imageable, though shorter (in terms of letters and syllables). Fry, Fountas and Pinnell, and Wonders tend to be more moderate than the others, exhibiting more instances where their distribution straddles the mean for all resources (and each showing no instance of being the extreme value on any given variable).

```{r profileFigureLexical, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Means and standard errors for a range of word variables within sources are shown. Means are calculated over standardized measures within WCBC, therefore the position of the point for a source for a variable can be interpreted as the average distance away from the mean of all words within WCBC (i.e., 0 shown with a red dashed line) for that variable in terms of standard deviation units. The mean on a given variable across all sources is shown as a grey dotted line just for convenience. Estimates were derived using t.test() in base R.', fig.width=7}

bardata = z_tables_lexical %>% 
  select(source = Source, se = `\\textit{SE}`, var, M = `\\textit{b}`) %>% 
  mutate(se = as.numeric(se),
         M = latex_to_num(M)) %>% 
  mutate(var = case_when(var == 'consistency' ~ 'Consistency',
                         var == 'syllables' ~ 'Syllables',
                         var == 'aoa' ~ 'AoA',
                         var == 'imageability' ~ 'Imageability',
                         var == 'letters' ~ 'Letters')) 
  
hlines = nsd %>% 
  filter(var %in% VARS) %>% 
  group_by(var) %>% 
  summarise(sourcemean = mean(M_, na.rm = T)) %>% 
  mutate(var = case_when(var == 'consistency' ~ 'Consistency',
                         var == 'syllables' ~ 'Syllables',
                         var == 'aoa' ~ 'AoA',
                         var == 'imageability' ~ 'Imageability',
                         var == 'letters' ~ 'Letters')) 

Zs %>% 
  group_by(source, var) %>% 
  summarise(source_order = first(source_order)) %>% 
  filter(var %in% VARS) %>% 
  mutate(var = case_when(var == 'consistency' ~ 'Consistency',
                         var == 'syllables' ~ 'Syllables',
                         var == 'aoa' ~ 'AoA',
                         var == 'imageability' ~ 'Imageability',
                         var == 'letters' ~ 'Letters')) %>% 
  left_join(bardata) %>% 
  ggplot(aes(reorder(source, source_order), M, color = reorder(source, source_order), group = reorder(var, source_order))) +
  geom_hline(data = hlines, aes(yintercept = sourcemean), color = 'black', linetype = 'dotted') +
  scale_color_manual(values = COLORS_SOURCE) +
  geom_point(size = 2) +
  geom_line(color = 'grey') +
  geom_errorbar(aes(ymin = M-se, ymax = M+se), size = .5, width = .8) +
  labs(color = 'Source', x = 'Variable', y = 'Standardized value (within WCBC)') +
  facet_grid(~var)  +
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') +
  theme_apa() +
  theme(legend.position = 'top',
        axis.ticks.x = element_blank(), 
        axis.text.x = element_blank(),
        text = element_text(family = 'Times'))

```

Also, the differences between the source distribution on each word-level variable relative to the normative distribution (WCBC) are all statistically significant based on a statistical test of the mean against zero (paralleling the tests conducted previously on the frequency distributions).

```{r profileTableStatTest, echo=FALSE, message=FALSE, warning=FALSE}
Source = z_tables_lexical %>% filter(var == 'aoa') %>% pull(Source)
AoA = z_tables_lexical %>% filter(var == 'aoa') %>% rename(b = `\\textit{b}`, se = `\\textit{SE}`) %>% mutate(o = paste(b, ' (', se, ')', sep = '')) %>%  pull(o)
Consistency = z_tables_lexical %>% filter(var == 'consistency') %>% rename(b = `\\textit{b}`, se = `\\textit{SE}`) %>% mutate(o = paste(b, ' (', se, ')', sep = '')) %>%  pull(o)
Imageability = z_tables_lexical %>% filter(var == 'imageability') %>% rename(b = `\\textit{b}`, se = `\\textit{SE}`) %>% mutate(o = paste(b, ' (', se, ')', sep = '')) %>%  pull(o)
Letters = z_tables_lexical %>% filter(var == 'letters') %>% rename(b = `\\textit{b}`, se = `\\textit{SE}`) %>% mutate(o = paste(b, ' (', se, ')', sep = '')) %>%  pull(o)
Syllables = z_tables_lexical %>% filter(var == 'syllables') %>% rename(b = `\\textit{b}`, se = `\\textit{SE}`) %>% mutate(o = paste(b, ' (', se, ')', sep = '')) %>%  pull(o)

data.frame(cbind(Source, AoA, Consistency, Imageability, Letters, Syllables)) %>% 
  mutate(Source = case_when(Source == 'Dolch' ~ 'Dolch',
                            Source == 'Fry' ~ 'Fry',
                            Source == 'Fundations' ~ 'Fundations',
                            Source == 'Kilpatrick' ~ 'Kilpatrick',
                            Source == 'Wonders' ~ 'Wonders',
                            Source == 'Fountas_pinnell' ~ 'Fountas & Pinnell')) %>%
  apa_table(caption = 'Model Estimates for Statistical Tests of Source Means against Means from Normative Sample', 
            note = 'Estimates obtained from using t.test() from base R, with standard errors in parentheses. Bolded parameter estimates are statistically significant. For a full accounting of parameter estimates see appendix. Values were Z transformed on the distribution from the WCBC prior to subsetting and testing by source.', escape = FALSE)
  
```

## Discussion
Words are known to vary in terms of a number of properties that contribute to learning to read [@Zevin2002; @Zevin2004; @Hsaio2018; @Steacy2019; @Weekes2006]. Instructional resources studied here vary in terms of the specific words they contain, however it could be that they nonetheless share a range of underlying properties, thus leaving these differences less meaningful. This turns out not to be the case, however. Noteworthy variability exists across the instructional wordlists along a range of variables, including word frequency, consistency, imageability, age-of-acquisition, number of letters, and number of syllables. This is true also for several different frequency norms: those derived from spoken language to young children, recreational texts for young readers, and texts for adult readers.

